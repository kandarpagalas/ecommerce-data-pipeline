{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f8cc05-0a14-46a2-aaae-50ba1e962d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession, functions as f\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import explode, col, date_format\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30054b5d-c764-4394-93d4-b5c1e0d6d378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a raiz do projeto\n",
    "os.chdir('/home/jovyan/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f64c2d-6321-48e5-a57e-c6b0bebc030b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7560b57-2f13-47a0-aeb9-557984c9266a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Minio\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT_\", \"minioserver:9000\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY_\", \"embuuWvqBMTLPRnbYXxu\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY_\", \"ZApLkWzj71pCNrB0IBGvQ5s5a2x4AJ42XSFZxb39\")\n",
    "\n",
    "# Postgres\n",
    "POSTGRES_DB = os.getenv(\"POSTGRES_DB\", \"z106\")\n",
    "POSTGRES_USER = os.getenv(\"POSTGRES_USER\", \"postgres\")\n",
    "POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\", \"z106pass\")\n",
    "POSTGRES_HOST = os.getenv(\"POSTGRES_HOST\", \"postgres_z106:5432\")\n",
    "\n",
    "# Remote\n",
    "MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"z106\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cab1f9a-3d5e-41c4-8989-a65da7e1e95d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 933 ms, sys: 369 ms, total: 1.3 s\n",
      "Wall time: 5min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"DW_z106\")\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.4\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e8e4e-eda9-4150-a924-c7f38a02433a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Carga de dados Dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf82497-f8ba-4caf-a7a4-9249d508155c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_postgres(spark, query):\n",
    "    df = (\n",
    "        spark.read.format(\"jdbc\")\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        .option(\"url\", f\"jdbc:postgresql://{POSTGRES_HOST}/{POSTGRES_DB}\")\n",
    "        .option(\"user\", POSTGRES_USER)\n",
    "        .option(\"password\", POSTGRES_PASSWORD)\n",
    "        .option(\"query\", query)\n",
    "        .load()\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe57db-7e6a-454a-9ba7-0b08c100b2ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a7abc16-fcb9-49b0-b54a-41bfc367962b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 ms, sys: 4.92 ms, total: 16.4 ms\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"SELECT * FROM customers\"\n",
    "df_customers = query_postgres(spark, query)\n",
    "\n",
    "# Criando a dimensão de clientes\n",
    "df_customers.createOrReplaceTempView(\"dm_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "498c8f0c-350b-449d-a372-64cc585305ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------------+--------------------+--------------+--------------------+--------------------+\n",
      "|                  id|created_at|             name|               email|        tax_id|             address|              phones|\n",
      "+--------------------+----------+-----------------+--------------------+--------------+--------------------+--------------------+\n",
      "|CID_77322b65-fb44...|2024-01-27|    Elisa Pacheco|elisa.pacheco@exe...|382.710.694-03|{'street': 'Aerop...|[{'country': '+55...|\n",
      "|CID_ea8cd53f-c02f...|2024-06-06|Valentina Costela|valentina.costela...|984.307.615-00|{'street': 'Estaç...|[{'country': '+55...|\n",
      "|CID_043bd699-82f3...|2022-07-19|    Igor Mendonça|igor.mendonça@exe...|538.092.647-92|{'street': 'Lotea...|[{'country': '+55...|\n",
      "|CID_d9a65e7f-441d...|2020-03-25|        Léo Silva|léo.silva@exemple...|308.496.715-66|{'street': 'Favel...|[{'country': '+55...|\n",
      "|CID_4e6ba3cd-8569...|2023-04-23|   Emanuel Farias|emanuel.farias@ex...|842.137.956-91|{'street': 'Parqu...|[{'country': '+55...|\n",
      "+--------------------+----------+-----------------+--------------------+--------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c87658-f6ed-4c83-b621-a1fbc3f76520",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Produtos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7558d6c-6957-410c-a442-17ceb55f03ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.62 ms, sys: 6.78 ms, total: 12.4 ms\n",
      "Wall time: 49.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"SELECT * FROM products\"\n",
    "df_products = query_postgres(spark, query)\n",
    "\n",
    "# Criando a dimensão de produtos\n",
    "df_products.createOrReplaceTempView(\"dm_products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6c34fe2-b295-4646-a077-052734a5020c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------+----------+\n",
      "| id|                name|           categoria|  reference_id|unit_price|\n",
      "+---+--------------------+--------------------+--------------+----------+\n",
      "|  1|         Cropped Mar|             cropped|     GQ6Z22YGT|     78.90|\n",
      "|  2|Top Z106 Caipirin...|colecao-brasilidades|   TP01-CAIPNH|     62.90|\n",
      "|  3|Short Moderninho ...|               short|    SH04-BOSSA|    112.90|\n",
      "|  4|      Top Z106 Brisa|                 top|     69X4JXZRX|     58.90|\n",
      "|  5|Top Liberdade Bri...|                 top|TP04-BRILHOTRP|     65.90|\n",
      "+---+--------------------+--------------------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e068ab-8730-4378-b1f0-23000d66b802",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98c6d8b7-6177-4fa3-97ba-f1da0fe6a4be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_date_df(spark, start_date, end_date):\n",
    "    date_list = [start_date + timedelta(days=x) for x in range(0, (end_date - start_date).days + 1)]\n",
    "    return spark.createDataFrame(date_list, DateType()).toDF(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e75b8f-e796-43a9-8bae-a2dec3ba66bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data de início e fim\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "\n",
    "# Gerando o DataFrame de datas\n",
    "df_dates = generate_date_df(spark, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94ff92b7-1ab0-4fbf-bd4a-6cdcc277d470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adicionando colunas de tempo\n",
    "df_time = df_dates.withColumn(\"year\", f.year(\"date\")) \\\n",
    "                  .withColumn(\"month\", f.month(\"date\")) \\\n",
    "                  .withColumn(\"day\", f.dayofmonth(\"date\")) \\\n",
    "                  .withColumn(\"weekday\", f.date_format(\"date\", \"E\")) \\\n",
    "                  .withColumn(\"week_of_year\", f.weekofyear(\"date\"))\n",
    "\n",
    "# Criando a dimensão de tempo\n",
    "df_time.createOrReplaceTempView(\"dm_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f5ff136-594c-4711-97d3-28f378535712",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+-------+------------+\n",
      "|      date|year|month|day|weekday|week_of_year|\n",
      "+----------+----+-----+---+-------+------------+\n",
      "|2023-01-01|2023|    1|  1|    Sun|          52|\n",
      "|2023-01-02|2023|    1|  2|    Mon|           1|\n",
      "|2023-01-03|2023|    1|  3|    Tue|           1|\n",
      "|2023-01-04|2023|    1|  4|    Wed|           1|\n",
      "|2023-01-05|2023|    1|  5|    Thu|           1|\n",
      "+----------+----+-----+---+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac87111c-0a3f-4944-8b68-c55b164bee28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Carga de dados Fato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbbb58fb-9105-43cd-aaa0-c20ada41e4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.9 ms, sys: 2.2 ms, total: 12.1 ms\n",
      "Wall time: 39.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"SELECT * FROM ft_orders\"\n",
    "ft_orders_df = query_postgres(spark, query)\n",
    "\n",
    "# Criando a dimensão de produtos\n",
    "ft_orders_df.createOrReplaceTempView(\"ft_orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cebe6e-1b98-4d55-a0b1-ba0fbe41260e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Salvando Dimensões como Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e41998f-acc9-4afb-8c58-43d8a3c93693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Salvar o DataFrame em formato Parquet no MinIO\n",
    "df_customers.write.mode(\"overwrite\").parquet(f\"s3a://{MINIO_BUCKET}/dw/dm_customers\")\n",
    "df_products.write.mode(\"overwrite\").parquet(f\"s3a://{MINIO_BUCKET}/dw/dm_products\")\n",
    "df_time.write.mode(\"overwrite\").parquet(f\"s3a://{MINIO_BUCKET}/dw/dm_time\")\n",
    "ft_orders_df.write.mode(\"overwrite\").parquet(f\"s3a://{MINIO_BUCKET}/dw/ft_orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca5657-2a12-401a-b406-1b207f2c63e7",
   "metadata": {},
   "source": [
    "## ToPandas timestamp fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5768f5-6ca2-4755-baed-aa6630c0ad20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf = df_orders.limit(10) \\\n",
    "    .withColumn(\"created_at\", date_format(\"created_at\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"charges_created_at\", date_format(\"charges_created_at\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"charges_paid_at\", date_format(\"charges_paid_at\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1479afdd-7ac8-45c9-b0ed-684f00ab6143",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o100.parquet.\n: java.lang.NoClassDefFoundError: com/amazonaws/AmazonClientException\n\tat java.base/java.lang.Class.forName0(Native Method)\n\tat java.base/java.lang.Class.forName(Class.java:467)\n\tat org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2625)\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2590)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonClientException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\t... 31 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Lendo os arquivos Parquet\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ft_orders_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMINIO_BUCKET\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/dw/ft_orders\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m customers_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3a://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMINIO_BUCKET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/dw/dm_customers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m products_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3a://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMINIO_BUCKET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/dw/dm_products\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:364\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    353\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    355\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    356\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    362\u001b[0m )\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o100.parquet.\n: java.lang.NoClassDefFoundError: com/amazonaws/AmazonClientException\n\tat java.base/java.lang.Class.forName0(Native Method)\n\tat java.base/java.lang.Class.forName(Class.java:467)\n\tat org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2625)\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2590)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonClientException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\t... 31 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51934)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lendo os arquivos Parquet\n",
    "ft_orders_df = spark.read.parquet(f\"s3a://{MINIO_BUCKET}/dw/ft_orders\")\n",
    "customers_df = spark.read.parquet(f\"s3a://{MINIO_BUCKET}/dw/dm_customers\")\n",
    "products_df = spark.read.parquet(f\"s3a://{MINIO_BUCKET}/dw/dm_products\")\n",
    "time_df = spark.read.parquet(f\"s3a://{MINIO_BUCKET}/dw/dm_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b139f04-324b-4c62-8610-4259a6f602ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Receita Total por Cliente (Top 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba654774-4732-44fd-b672-14ec81604550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "receita_por_cliente_df = ft_orders_df \\\n",
    "    .groupBy('customer') \\\n",
    "    .agg(f.sum('charge_value').alias('total_revenue')) \\\n",
    "    .orderBy('total_revenue', ascending=False) \\\n",
    "    .limit(10)\n",
    "\n",
    "receita_por_cliente = receita_por_cliente_df.toPandas()\n",
    "clientes = receita_por_cliente['customer'].fillna('Desconhecido')  # Substituir valores None\n",
    "receitas = receita_por_cliente['total_revenue']\n",
    "receita_por_cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa07e2-6c22-4ad0-9f2a-1665116a33f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Número de Pedidos por Categoria de Produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47af6ab-74dd-4812-88ef-3fddc717989a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items_df = ft_orders_df.withColumn('item', explode(col('item_references')))\n",
    "items_df = items_df.select('item', 'charge_value')\n",
    "pedidos_por_produto_df = items_df.join(products_df, items_df.item == products_df.reference_id, 'left') \\\n",
    "    .groupBy('category') \\\n",
    "    .agg(f.count('item').alias('order_count'))\n",
    "\n",
    "categorias_df = pedidos_por_produto_df.toPandas()\n",
    "categorias = categorias_df['category'].fillna('Desconhecida')  # Substituir valores None\n",
    "quantidade_pedidos = categorias_df['order_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2550c-a9f1-4cf1-ab1b-006b1e8ed48c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Receita Total por Mês\n",
    "receita_mensal_df = ft_orders_df \\\n",
    "    .join(time_df, ft_orders_df.created_at.cast('date') == time_df.date, 'left') \\\n",
    "    .groupBy('year', 'month') \\\n",
    "    .agg(f.sum('charge_value').alias('total_revenue')) \\\n",
    "    .orderBy('year', 'month')\n",
    "\n",
    "receita_mensal = receita_mensal_df.toPandas()\n",
    "\n",
    "# Garantir que os valores de year e month são inteiros\n",
    "receita_mensal['month'] = receita_mensal['month'].fillna(0).astype(int)\n",
    "receita_mensal['year'] = receita_mensal['year'].fillna(0).astype(int)\n",
    "\n",
    "# Criar o período no formato 'YYYY-MM'\n",
    "receita_mensal['period'] = receita_mensal.apply(lambda x: f\"{int(x['year']):04d}-{int(x['month']):02d}\", axis=1)\n",
    "meses = receita_mensal['period']\n",
    "receitas_por_mes = receita_mensal['total_revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113d38a-557b-49e6-855a-920846b96265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Número de Pedidos por Estado de Entrega\n",
    "shipping_df = ft_orders_df.withColumn(\n",
    "    'shipping_region_code',\n",
    "    f.get_json_object(f.col('shipping'), '$.region_code')\n",
    ")\n",
    "quantidade_pedidos_por_estado_df = shipping_df \\\n",
    "    .groupBy('shipping_region_code') \\\n",
    "    .agg(f.count('id').alias('order_count'))\n",
    "\n",
    "estados_df = quantidade_pedidos_por_estado_df.toPandas()\n",
    "estados = estados_df['shipping_region_code'].fillna('Desconhecido')  # Substituir valores None\n",
    "quantidade_pedidos_por_estado = estados_df['order_count']\n",
    "\n",
    "# Criar subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Receita Total por Cliente (Top 10)\n",
    "axs[0, 0].bar(clientes, receitas, color='skyblue')\n",
    "axs[0, 0].set_title('Receita Total por Cliente (Top 10)')\n",
    "axs[0, 0].set_xlabel('Cliente')\n",
    "axs[0, 0].set_ylabel('Receita Total (BRL)')\n",
    "axs[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Número de Pedidos por Categoria de Produto\n",
    "axs[0, 1].bar(categorias, quantidade_pedidos, color='lightgreen')\n",
    "axs[0, 1].set_title('Número de Pedidos por Categoria de Produto')\n",
    "axs[0, 1].set_xlabel('Categoria do Produto')\n",
    "axs[0, 1].set_ylabel('Número de Pedidos')\n",
    "axs[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Receita Total por Mês\n",
    "axs[1, 0].plot(meses, receitas_por_mes, marker='o', linestyle='-', color='coral')\n",
    "axs[1, 0].set_title('Receita Total por Mês')\n",
    "axs[1, 0].set_xlabel('Mês')\n",
    "axs[1, 0].set_ylabel('Receita Total (BRL)')\n",
    "axs[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Número de Pedidos por Estado de Entrega\n",
    "axs[1, 1].bar(estados, quantidade_pedidos_por_estado, color='lightcoral')\n",
    "axs[1, 1].set_title('Número de Pedidos por Estado de Entrega')\n",
    "axs[1, 1].set_xlabel('Estado de Entrega')\n",
    "axs[1, 1].set_ylabel('Número de Pedidos')\n",
    "axs[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Ajustar layout e mostrar\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74dd999-2411-4bb4-bb40-14f3aa41dd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
