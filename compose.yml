---
networks:
  z106-net:
    driver: bridge

services:
  kafka:
    image: apache/kafka:latest
    container_name: kafka
    networks:
      - z106-net
    restart: unless-stopped
    ports:
      - 19092:19092
    env_file:
      - .env
    volumes:
      - ./data/volumes/kafka/logs:/tmp/kraft-combined-logs
      # - /var/run/docker.sock:/var/run/docker.sock
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_CREATE_TOPICS: "orders"

      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,CONNECTIONS_FROM_HOST://localhost:19092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONNECTIONS_FROM_HOST://localhost:19092,CONTROLLER://kafka:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONNECTIONS_FROM_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      # KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      # KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      # KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      # KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      # KAFKA_NUM_PARTITIONS: 3

      KAFKA_LOG_RETENTION_HOURS: 1
      KAFKA_LOG_RETENTION_BYTES: 4073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      # KAFKA_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'

  spark:
    image: bitnami/spark:3.5.1
    container_name: spark_master
    networks:
      - z106-net
    env_file:
      - .env
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - '18080:8080' # GUI
      - '7077:7077' # API
    volumes:
      - ./data/volumes/spark/content:/content
      - ./data/volumes/spark/jars:/jars
      - ./src/streaming/consumers:/consumer
      # - /var/run/docker.sock:/var/run/docker.sock

  spark_worker:
    image: bitnami/spark:3.5.1
    container_name: spark_worker
    networks:
      - z106-net
    env_file:
      - .env
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      # - SPARK_WORKER_MEMORY=3G      #<--- adjust accordingly
      # - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    volumes:
      - ./data/volumes/spark/content:/content
      - ./data/volumes/spark/jars:/jars
      - ./src/streaming/consumers:/consumer
      # - /var/run/docker.sock:/var/run/docker.sock

  postgres:
    container_name: postgres_z106
    image: postgres:14-alpine
    networks:
      - z106-net
    restart: unless-stopped
    ports:
      - 35432:5432 # API
    volumes:
      - ./data/volumes/postgres/data:/var/lib/postgresql/data
    env_file:
      - .env
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}

  redis:
    image: redis
    container_name: redis_z106
    networks:
      - z106-net
    command: redis-server --requirepass ${REDIS_PASSWORD}
    restart: unless-stopped
    ports:
      - "36379:6379" # API
    volumes: 
      - ./data/volumes/redis/data:/data

  minioserver:
      image: minio/minio
      container_name: minioserver
      networks:
      - z106-net
      ports:
        - 9000:9000 # API
        - 9001:9001 # GUI
      env_file:
      - .env
      environment:
        MINIO_ROOT_USER: ${MINIO_ROOT_USER}
        MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      volumes: 
      - ./data/volumes/minio/data:/data
      command: server /data --console-address ":9001"

  painel:
    build:
      context: .  # This should be the root directory of your project
      dockerfile: docker/streamlit-spark/Dockerfile  # Path to your Dockerfile
    image: streamlit/spark  # Name the image "my_image"
    container_name: painel
    networks:
      - z106-net
    ports:
      - 8501:8501
    volumes:
      - ./:/app
    env_file:
      - .env
    environment:
      PRODUCER_MAX_INTERVAL: ""
    depends_on:
      - kafka
    # command: ["streamlit", "run", "src/pages/kafka_orders_producer_gui.py", "--server.port=8501", "--server.address=0.0.0.0"]

  producer:
    build:
      context: .  # This should be the root directory of your project
      dockerfile: docker/python-kafka/Dockerfile  # Path to your Dockerfile
    image: python/kafka  # Name the image "my_image"
    container_name: producer_orders
    depends_on:
      - kafka
    networks:
      - z106-net
    volumes:
      - ./:/app
    environment:
      KAFKA_TOPIC: orders
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      # LIMIT_NUM_ORDERS: 100 # Limita a quandidade de vendas produzidas
      PRODUCER_MAX_INTERVAL: 5 # Limita o intervalo entre vendas
    command: ["python3", "src/streaming/producers/orders_producers.py"]
